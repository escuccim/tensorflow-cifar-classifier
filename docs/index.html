<!DOCTYPE html>
<html>
<head>
<title>CIFAR Classifier with TensorFlow</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
</head>

<body>
<div class="container">
    <div class="row">
        <div class="col-md-12">
            <h3>Convolutional Neural Networks with TensorFlow for Classifying CIFAR10 Data</h3>
            <p>Eric Scuccimarra (skooch@gmail.com)</p>
        </div>
        <div class="col-md-12">
            <h4>Summary</h4>
            <hr>
            <p>I created and tested several convolution neural network models with TensorFlow to classify the CIFAR 10 data. The process and results are described below.
        </div>
        <div class="col-md-12">
            <h4>Initial Models</h4>
            <hr>
            <p>The initial models are detailed <a href="#models">below.</a> Model 1 was very similar to the model described in the <a href="https://www.tensorflow.org/tutorials/deep_cnn">TensorFlow Deep CNN tutorial</a>, with the local response normalization replaced with batch normalization.
            
        </div>
        <div class="col-md-12">
            <h4>Model Selection</h4>
            <p>The initial models were evaluated for 50 epochs each and then iteratively improved. The worst performing models were discarded and the best performing models altered to see if their performance could be improved. After evaluating about 70 different models and variations on those models, the final model selected was model_7.20.2.13v-64.
            <p>It soon became impractical to train the models on my laptop, so I trained them on Google CoLab (https://colab.research.google.com) where the models were trained in about 1/6 of the time it would have taken on my laptop.
            
            <h4>Optimizing the Model</h4>
            <p>Once an architecture was selected the process of attempting to optimize it began. During the optimization process the following items were tuned:
            <ul>
                <li>The size of the convolutional filters
                <li>The size and type of the max pools
                <li>The type and strength of regularization
                <li>The learning rate
            </ul>
            
            <p>Among the different techniques evaluated were:
            <ul>
                <li>Batch normalization
                <li>Fractional max pooling
                <li>Dropout
                <li>All-convolutional layers
            </ul>
            
            <h4>Best Performing Model</h4>
            <p>Convolutional filters of size 5x5, 4x4 and 3x3 were evaluated. In the end using a 5x5 filter for the first two convolutional layers and 3x3 for the remaining layers provided the best results.
            <p>The batch normalization was the most important factor in improving the performance of the network. Adding it to the fully-connected layers provided an immediate, noticeable improvement in both accuracy and speed of learning. It was used after every convolutional and fully connected layer.
            <p>The fractional max pooling provided a comparatively minor improvement in accuracy, at the expense of increasing the size of the model and a dramatic increase in training time. In the end a fractional max pool with a size of 1.44x1.44 was used to replace the first max pooling layer. The second pooling layer was a max pool with a size of 3x3 and a stride of 2, and the last pooling layer was a max pool with size 2x2 and stride 2. Using a fractional max pool increased training time by about a factor of 2 and increased the size of the model by about 1.8 while providing about a 2% increase in accuracy.
            <p>Dropout was also used with a rate of 10% applied to some, but not all, of the convolutional and pooling layers, and a rate of 25% applied to the fully connected layers.
            
            <h4>Hyperparameters</h4>
            <p>L2 regularization was used on all layers, with a lambda of 0.000050 applied to the convolutional layers and a lambda of 0.01 applied to the fully connected layers.
            <p>The learning rate was also tuned with the goal of decreasing training time to allow more options to be evaluated. The final learning rate was an exponential decay:
            <ul>
                <li>Starting Rate: 0.002
                <li>Epochs per Decay: 10
                <li>Decay Factor: 0.90
                <li>Staircase: True
            </ul>
            
            <h4>Results</h4>
            <p>The best result to date is an accuracy of .9218 on the test set and .9236 on the validation set, reached after 350 epochs of training.
            <p>The results of each model were logged in an Excel file which is available in the GitHub repo.
            
        </div>
        <div class="col-md-12">
            <a name="final_model"></a>
            <h4>Final Model</h4>
            <hr>
            <h5>Model 7.20.2.13v</h5>
            <div class="row">
                <div class="col-md-4">
                    <ol>
                        <li>Conv 1
                            <ol>
                                <li>64 5x5 units with a stride of 1 and L2 regularization</li>
                                <li>Batch normalization</li>
                                <li>Relu activation</li>
                                <li>Dropout at 10%</li>
                            </ol>
                        </li>
                        <li>Conv 2
                            <ol>
                                <li>64 5x5 units with a stride of 1 and L2 regularization</li>
                                <li>Batch normalization</li>
                                <li>Relu activation</li>
                            </ol>
                        </li>
                        <li>Fractional Max Pool 1 size 1.44x1.44 
                            <ol>
                                <li>Dropout at 10%</li>
                            </ol>
                        </li>
                        <li>Conv 3
                            <ol>
                                <li>96 3x3 units with a stride of 1 and L2 regularization</li>
                                <li>Batch normalization</li>
                                <li>Relu activation</li>
                                <li>Dropout at 10%</li>
                            </ol>
                        </li>
                        <li>Conv 4
                            <ol>
                                <li>96 3x3 units with a stride of 1 and L2 regularization</li>
                                <li>Batch normalization</li>
                                <li>Relu activation</li>
                            </ol>
                        </li>
                        <li>Max Pool 2 size 3x3 with stride of 2
                        </li>
                        <li>Conv 5
                            <ol>
                                <li>128 3x3 units with a stride of 1 and L2 regularization</li>
                                <li>Batch normalization</li>
                                <li>Relu activation</li>
                                <li>Dropout at 10%</li>
                            </ol>
                        </li>
                        <li>Conv 6
                            <ol>
                                <li>128 3x3 units with a stride of 1</li>
                                <li>Batch normalization</li>
                                <li>Relu activation</li>
                            </ol>
                        </li>
                        <li>Max Pool 3 size 2x2 with a stride of 2</li>
                        <li>Flatten output
                            <ol>
                                <li>Dropout at 10%</li>
                            </ol>
                        </li>
                        <li>Fully connected layer 1 with 1024 units
                            <ol>
                                <li>Batch normalization
                                <li>Relu activation
                                <li>Dropout at 25%
                            </ol>
                        <li>Fully connected layer 2 with 512 units
                            <ol>
                                <li>Batch normalization
                                <li>Relu activation
                                <li>Dropout at 25%
                            </ol>
                        </li>
                        <li>Logits</li>
                        <li>Loss = cross entropy + regularization loss
                        <li>Adam Optimizer minimizing loss
                    </ol>
                </div>
                <div class="col-md-8">
                    <img src="model_7.20.0.png" style="width: 100%;">
                </div>
            </div>
        </div>
        <div class="col-md-12">
            <a name="models"></a>
            <h4>Initial Models</h4>
            <hr>
            <h5>Model 1</h5>
            <p>This is essentially the same model described in the TensorFlow <a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank">deep CNN tutorial.</a> A few minor changes have been made.
            <div class="row">
                <div class="col-md-4">
                    <ol>
                        <li>Conv 1
                        <ol>
                        <li>64 5x5 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 1 size 3x3 with stride 2
                        <ol>
                        <li>Dropout at .125</li>
                        </ol>
                        </li>
                        <li>Conv 2
                        <ol>
                        <li>64 5x5 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 2 size 3x3 with stride 2
                        <ol>
                        <li>Dropout at 0.125</li>
                        </ol>
                        </li>
                        <li>Flatten output
                        <ol>
                        <li>Dropout at 0.25</li>
                        </ol>
                        </li>
                        <li>Fully connected layer 1 with 384 units
                        <ol>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Fully connected layer 2 with 192 units
                        <ol>
                        <li>Dropout at 0.25</li>
                        </ol>
                        </li>
                        <li>Logits</li>
                    </ol>
                </div>
                <div class="col-md-8">
                    <img src="model_1.png" style="width: 100%;">
                </div>
            </div>
        </div>
        <div class="col-md-12">
            <h5>Model 2</h5>
            <div class="row">
                <div class="col-md-4">
                    <ol>
                        <li>Conv 1
                        <ol>
                        <li>32 3x3 units with a stride of 2</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Conv 2
                        <ol>
                        <li>32 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 1 2x2 with stride of 1
                        <ol>
                        <li>Dropout at 0.125</li>
                        </ol>
                        </li>
                        <li>Conv 3
                        <ol>
                        <li>64 3x3 units with stride of 2</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Conv 4
                        <ol>
                        <li>64 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 2 3x3 with stride of 2
                        <ol>
                        <li>Dropout at 0.125</li>
                        </ol>
                        </li>
                        <li>Flatten output
                        <ol>
                        <li>Dropout at 0.125</li>
                        </ol>
                        </li>
                        <li>Fully connected layer 1
                        <ol>
                        <li>512 units with relu activation</li>
                        <li>Dropout at 0.25</li>
                        </ol>
                        </li>
                        <li>Logits</li>
                    </ol>
                </div>
                <div class="col-md-8">
                    <img src="model_2.png" style="width: 100%;">
                </div>
            </div>
        </div>
        <div class="col-md-12">
            <h5>Model 3</h5>
            <div class="row">
                <div class="col-md-4">
                    <ol>
                        <li>Conv 1
                        <ol>
                        <li>32 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Conv 2
                        <ol>
                        <li>32 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 1 2x2 with stride of 2
                        <ol>
                        <li>Dropout at 0.1</li>
                        </ol>
                        </li>
                        <li>Conv 3
                        <ol>
                        <li>64 3x3 units with stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        <li>Dropout at 0.1</li>
                        </ol>
                        </li>
                        <li>Conv 4
                        <ol>
                        <li>64 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 2 2x2 with stride of 2</li>
                        <li>Conv 5
                        <ol>
                        <li>128 3x3 units with stride of 1</li>
                        <li>Relu activation</li>
                        <li>Dropout at 0.1</li>
                        </ol>
                        </li>
                        <li>Conv 6
                        <ol>
                        <li>128 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 3 2x2 with stride of 2</li>
                        <li>Flatten output
                        <ol>
                        <li>Dropout at 0.1</li>
                        </ol>
                        </li>
                        <li>Fully connected layer 1
                        <ol>
                        <li>1024 units with relu activation</li>
                        <li>Dropout at 0.1</li>
                        </ol>
                        </li>
                        <li>Logits</li>
                    </ol>
                </div>
                <div class="col-md-8">
                    <img src="model_3.png" style="width: 100%;">
                </div>
            </div>
        </div>
        <div class="col-md-12">
            <h5>Model 4</h5>
            <div class="row">
                <div class="col-md-4">
                    <ol>
                        <li>Conv 1
                        <ol>
                        <li>32 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Conv 2
                        <ol>
                        <li>32 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 1 2x2 with stride of 2
                        <ol>
                        <li>Dropout at 0.125</li>
                        </ol>
                        </li>
                        <li>Conv 3
                        <ol>
                        <li>64 3x3 units with stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Conv 4
                        <ol>
                        <li>64 3x3 units with a stride of 1</li>
                        <li>Batch normalization</li>
                        <li>Relu activation</li>
                        </ol>
                        </li>
                        <li>Max Pool 2 2x2 with stride of 2
                        <ol>
                        <li>Dropout at 0.125</li>
                        </ol>
                        </li>
                        <li>Fully connected layer 1
                        <ol>
                        <li>512 units with relu activation</li>
                        <li>Dropout at 0.25</li>
                        </ol>
                        </li>
                        <li>Logits</li>
                    </ol>
                </div>
                <div class="col-md-8">
                    <img src="model_4.png" style="width: 100%;">
                </div>
            </div>
        </div>
        <div class="col-md-12">
            <h4>References and Notes</h4>
            <ol>
                <li>Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015)  arXiv:1502.03167v3 [cs.LG], URL <a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>
                <li>Srivastava, et al, Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), Journal of Machine Learning Research 15 (2014) 1929-1958, URL <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer">http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer</a>
                <li>Benjamin Graham, Fractional Max-Pooling (2105) arXiv:1412.6071v4, URL: https://arxiv.org/pdf/1412.6071.pdf
                <li>Springenberg, et al, Striving for Simplicity: The All Convolutional Net (2015), arXiv:1412.6806v3, URL: https://arxiv.org/pdf/1412.6806.pdf
            </ol>
        </div>
    </div>
</div>

</body>
</html>