{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "\n",
    "X_test = test_data[b'data']\n",
    "X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:])) / 255.0\n",
    "X_test = X_test.reshape(-1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x292ef8c5358>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x292efa99fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x292efa52ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJztnXl0XPWV57+3SqpSabck2xK2bHkDbxjbeGOzDThASAI4J6EhGXACidMdyBmmOyQccnogMz2d0DOQJp004AQOpJuGsMMBwmYM2CwGecH7bsmbbEmWJWstqaru/KFyjxG/b0lYdsnM737O8ZF8v3Xf++nVu/Wq3q17r6gqDMPwj8BAL8AwjIHBgt8wPMWC3zA8xYLfMDzFgt8wPMWC3zA8xYLfMDzFgt8wPMWC3zA8JaM/ziJyBYD7AQQB/FFVf53q8Xl5eVpcMtipBZCgfvw7iJJicfx1LdW+Eim+8ZhIEL9ECp8Uq493BamWGeFayldsca9RlXsFAymOYyJOpVicawmyPyHrAwBNta9gJtXCwVTPNfnbNMU5kOIIB1IcK+1qo1pbW4xqkhl22jMzUj0v7u01NDahtbUtheP/44SDX0SCAH4P4CsA9gH4REReUtVNzKe4ZDDuuusfnFqWtNN9xciJFA+keJIyI1SLSJRqbdFOqrW2dbiFKLED6AA/oZv2F1CtbFI+1XJSBBCC7rVEY+4TDAAKcrOopq1HqNbQ1ES11li20x7O4Mc+1s73dbDgDKqdVcif68wEedGI83V0Kj8e2Vk8ZDoOrqHamk/rqZYxZJTTXl7C1xFrq3Xa73/wUerTk/687Z8FYIeq7lLVTgBPAri6H9szDCON9Cf4hwHYe9z/9yVthmF8CehP8Ls+V3zuA66ILBaRShGpbGlu7sfuDMM4mfQn+PcBKD/u/8MBHOj5IFVdoqozVHVGbl5eP3ZnGMbJpD/B/wmAcSIySkRCAK4D8NLJWZZhGKeaE77br6oxEbkVwOvoTvU9oqobe3FCPNbllLJyc6hbV8B9dzuW4HeAc9v2Uq1Z+MePj1auolrLoYNOe2fwLOoz5Tx+GyQ7J0WqL8CfmnAgRfqQJHly/mok9an94ydU2/LWh1QLjHanbQGg8XCV097cwdcxeeZwqmXHG6i2r577jSlxX98S4KnDoR3bqPbsQ+9TrXroNKrNGcHf9e6rc4fNR58Moj7fvJyc+8LPqZ70K8+vqq8CeLU/2zAMY2Cwb/gZhqdY8BuGp1jwG4anWPAbhqdY8BuGp/Trbv8XpbP9EPat+41TWzr5F9TvmkGkQiy+gfqs3fMp1Zav2Ue1gllXUm1CYKvT3l6/mvo8tbyYaotm82KmD+95mmqT/34R1coy3IVEu//HMuqTEaO1WKgP86KlUeeOoFrrh+4CKdn6EfV5fcj1VLt63k6qPbeuhGq3FLpTehk5/Dlb2TGJaqUlPGXX3L6daoXDKqi2c4c7/Z3X+hb1ebvr+0770S8whsOu/IbhKRb8huEpFvyG4SkW/IbhKRb8huEpab3bn5OZjZll5zi1YYNbqF9LnbtgYuEsXqCT1VhGtR1Zn6s8/k92fcTvwF94nvtW6ocNvMClfO+/U23dTHfmAwDyfjCaaqNbeKuxledf5bTfFP2Y+ryzlfeea+56hWq7c6+h2nxxt7SqHF3utAPAgbdfpFrVwglUWziFH6vMoLtoaU3JtdRnZg4/HjtG8swCXhxKpSuu4efIehx22keVu9t7AcCa15c77fGjPI56Yld+w/AUC37D8BQLfsPwFAt+w/AUC37D8BQLfsPwlLSm+gLtIUQ2uNMXg8fz6STB0G+d9je3jqc+RQvGUG3EPp7K2TjiPKrl7l/ntCfivP9gzmierml8P0S1xNU8pfTBYZ7Omfrb/3DaX5k/kfosnM3TUHs6eJ/E4Tm86GdzvXsiTnWU72vCNF4E1Rbi58eowueo9uaeuU67fOBOlQHA8K/x1GHNuMlUKx7Fn8/qQ+4JOwAwKOAuxmoq5c/Z2bnu6/bOjbwfY0/sym8YnmLBbxieYsFvGJ5iwW8YnmLBbxieYsFvGJ7Sr1SfiFQBaAYQBxBT1RkpH58VQdbEs51a14Y/Ub8NZeOc9raaG6jPbYceo9pKcaehAODsTl7xF2+NOe3NwXzqM2EU73MXKeR+ZYU8nTfkVd6fcM+Fe5z2wlAB9XlqKe9pWNPBq+nO3sMbxg0e4h6xFm7i15vwSH76TC7JpVqsmvfV61pR77TPyubVmy+8zPsMNor7+AKAfrWCavc38uMfrtnttMcG8crUK2a6KwhfW8rHkPXkZOT5L1ZV9xE2DOO0xd72G4an9Df4FcAbIrJKRBafjAUZhpEe+vu2/wJVPSAiQwC8KSJbVPW94x+QfFFYDABDC1J0QTEMI63068qvqgeSP2sBPA9gluMxS1R1hqrOKMzhN7gMw0gvJxz8IpIjInnHfgdwGQA+QscwjNOK/rztHwrgeRE5tp3/UNXXUjkEsxUFU9yjiQ4mplC/j3e6mz5O2/4j6vO7Ep42KtzRSLXZOauotn77Iad9fxtPec1dwMd/DY65U4cAMCp4hGptl2yh2uHac5320EPudBIAVH3tcqoVvMarxHaVv0+1Zz6sdtrP+sq3qc/8s9znBgDUrqmgWkHuUaqdH6xx2ovjO6jPW3W8yehNN82n2ktL3qVax0Z3Q1MA2IAhTvu3Jg6nPk2HDjrt8S5+TvXkhINfVXcBcLfiNQzjtMdSfYbhKRb8huEpFvyG4SkW/IbhKRb8huEposrTVCeb0vFj9cY/3ufULjq6lfo9Vn2L0/7rI/dTnzcmzaHa2R3LqPbM790NMAHg5RZ3JdWwSIoGnim+1Nh8KJtqP/3596kW2LWUaqP/m3v+X/yun1GfZzp4c8z6tXzG34bGBqq1Z5Bmls08nZeVyRuCHonOpNriu75JtcHxBU77dXPvoD7zvsmbe7amSKVFE+5GnAAQS5FYyySbDGZFqE9Y3E1oN27egdbWNqGOx2FXfsPwFAt+w/AUC37D8BQLfsPwFAt+w/CUtN7tD4VytbR0qlNbfM9Xqd+8637itH8y72Lq83iKYhvEeA+/1mauISPsNIeK+C39fPBefM1d7j53ANB4hPdvu+lGd8YEAO66wJ0Zmf1zXvySSHE82rt4r7uY8jvYGSH36K3iIuqCjqM8a1Ie2Em1ytaRVPv+X9yjvGLf4efOWzGekWiJ8uulxvjzKQGe2cmPuLMcTQ28H9+Iae7z4/23lqOpodHu9huGwbHgNwxPseA3DE+x4DcMT7HgNwxPseA3DE9Ja6ovJyuk44eXOrWmyDTq98QT7rTRz2/mfdGinbxIpFl5IUtAg1TriLoLN7LABxZFh59FtVAj79PXephroXzec++XRaud9n9qdacpASAkvKdhYzPPGoVZRQqA9ihJl2WewbfXvJZqGeOGUa1+D0/N5WaMdtpDrbzXbGsR7+FXkqLxXSCDF/a0tfBU65G4u2BsyshO6rN5hfs5q26tQke83VJ9hmFwLPgNw1Ms+A3DUyz4DcNTLPgNw1Ms+A3DU3pN9YnIIwC+DqBWVScnbUUA/gygAkAVgGtVleemkuRlZ+m08e4RRA1tPBUlJHFx5AivmEOI9JADUDKMp5uKi/k6jlQdcNpzJl5CfUbVPEu1d7bwcUzDylqp1tjFU0qJmDul1Bjlz3O4jVf1BYdPotrEYp5Ora1zD2Udd6F7NBUAbH5+JdX2X/cNqi0K8cxWNO7WAgme0kWCby/Es8SIdfCUYyxFpWAc7pReRxtPpcY73ef+y395C/WHG05aqu9RAFf0sN0BYKmqjgOwNPl/wzC+RPQa/Kr6HoCebVqvBvBY8vfHAFxzktdlGMYp5kQ/8w9V1RoASP7k7+UMwzgt6c+I7j4hIosBLAaAcOYp351hGH3kRK/8h0SkDACSP2vZA1V1iarOUNUZmRkpbrIYhpFWTjT4XwKwKPn7IgAvnpzlGIaRLvqS6nsCwHwAJQAOAbgLwAsAngIwAsAeAN9WVT67KcnQoaX6neu+69QyAnwdEnC/Y4jGedVTMIun7AIx7hcHX0dHs3tEkqR4De1I0Ryzs62AakVDebYmzkZhAciBu4lkvJOnjSSTr1/I9gCg/uBuqh3ucufEiovyqE8wxSistgg/ViW8zyWyyMirRJx/BB2Zx1PI1Sl6Y8Z21VCtLptvc399s9M+ZPh46lMo7jTr8y++grq6+j6l+nr9EK6q1xPp0r7swDCM0xP7hp9heIoFv2F4igW/YXiKBb9heIoFv2F4Slq/cqcCdGWQKqsUKbZQwF3FFsr/a+oz/uhDVFu76wOqbR16HtXOJtVXmw7WUZ+cIl65l5PJ/+auVv66nJPH/RKkIi2jnM/Ba3z3NaptODyKarMvnky1khr3975q1m+iPi3n8NRWTrSJaq/t5t8uXzjBfawCwisjt1Zxrb6ap/NaBp1DtUuHV1Mtd7A7LVr9zmbqkzejwmnXFHHUE7vyG4anWPAbhqdY8BuGp1jwG4anWPAbhqdY8BuGp6Q11ReINSGrwZ1WWlVwJfWbHnC/RunhX1OfdUNyqVZXMIZqWQW8YWV2jbvB5OCguyoLALas4cWO06dfRTXZtIVqkdm8+i2RXeS0797NKwFnjR1Btbps3nhyUwav0Ks4st5pby/gqahP1/HumAtu5ynHebXZVEtsc6fY5t90IfVZs5zPgAw2H6Ra8773qHZk2hyqxSrdTVfzy3kl4FsH3X9zc1ffr+d25TcMT7HgNwxPseA3DE+x4DcMT7HgNwxPSevd/vyMIVhQcqtTmzV+K/VbtfTbTvuPF/O7q8s+3k+1tYf5mKmlbTxLcNlc92vl5vW8eOeM2kqqfdjoPhYAMPda97grAJh0kG/z1QPu4pJvnsOLj3BoGJUG6ZtUG/91nqEp2uHuu9hWxjML41etotqnRe7+iQBwVQa/c99a3e60//OKHdRnSt7lVDtrOJ9Kl7FzHtXmFu6l2sOkf2V2zpnUZ1LzWqe9Ud1/rwu78huGp1jwG4anWPAbhqdY8BuGp1jwG4anWPAbhqf0muoTkUcAfB1ArapOTtruBvBDAMfyR3eq6qu9bismiNS5U0CZnXyEVmjY/3Ta//3tb1Cfid8dR7WSet7Db9iYQqrFN7n7yMX38uKXwRNnUC17H58zpUN5z70PVvLCnvKml532tZN4Wm5iYizVzjzMU2KFBw5T7WAJKSRqHkp9Jkzio8HqgrywZ1SCr+OZuPv5PGMXL8a65afvUO3x9XyNbWfybe5L8FRlvNUdhpFxvDdhXmSQ0x7M3EV9etKXK/+jAK5w2H+jqlOT/3oNfMMwTi96DX5VfQ9Ar0M4DcP4ctGfz/y3isg6EXlERNzvQQzDOG050eB/AMAYAFMB1AC4lz1QRBaLSKWIVDZ18M9EhmGklxMKflU9pKpxVU0A+AOAWSkeu0RVZ6jqjIIs3vnFMIz0ckLBLyJlx/13IYANJ2c5hmGki76k+p4AMB9AiYjsA3AXgPkiMhWAAqgC8KO+7CyYHUDOue7eY3Wl9XwNG6Y57ZWDeOVbyTNvU62mkafmits3Ui1eUuK0x6p5r7X2hhT7GllMtfPBt1mYd4hqu6e6R03tfuEl6rPiKO+ddzSXjy/LvId/jBuRt8dp3y5h6iMTR1KtMH821bIaqYSpVe5egoPG8z59N987mGozh/C0YkcRT92uXst7Mh6odqe5G0fx43FusXssW5iMw3PRa/Cr6vUO88N93oNhGKcl9g0/w/AUC37D8BQLfsPwFAt+w/AUC37D8JS0NvCMRwJoneRukBmP8uaHO7tqnPY57Q9Sn107v0a1ywa/T7UPD/NmnCtf+dhp3z9hPPWZOLKMatGjfB3Dcvg3pltm83TZ0SL36/mBFe6KRAAY/r1rqDbpL/wYv555gGqrd7jTXmWXVlCf8jhPbzY8zqstg9N4c9I8daeDb75xOvWJvj2RarPH8uq8d5/9LdWeWc/TqRWT3dV7RTH+nB085B7x1dXFKz57Yld+w/AUC37D8BQLfsPwFAt+w/AUC37D8BQLfsPwFFF1zwk7FQwZf6Z+6xF3OuTS+u3Ub8XoHzvt9+b9C/X51bM8JSNHeIXVuk181lk8GHHas0PupqQAkD2IV1lFm/nMwPFfdc8nBICzA9xv3sKLnfZ/ueFvqM+KFOsIhtx/MwAEM7kWFvd5lRHmacog+LnYGedrzJ37X6g2eqx77t5P9vyA+ix6hZ8DCv58SiLK/TJIQ1MAoVCR054f4ccjFnWv48VXnkVdfV2fSvvsym8YnmLBbxieYsFvGJ5iwW8YnmLBbxiektbCHqmvR+ihR5za27POpX63fN1dUPO9G1dSn7jy3nkdHXzkUkJ4H7ZI2P1amV+UoitxghdaFE/kxTvbnnucapsu4AUk8353u9O+JuQuqAKAUJhnKwLCtYwMfud+UIm7kKW9yV2QAgA5Y87h26vnGZodSx+l2p5L3P0f73mauiCYx6+J0SZ3j0QACIfd/SkBIDvIQy0z4s5MNYCPUZuQsdu9rRQZk57Yld8wPMWC3zA8xYLfMDzFgt8wPMWC3zA8xYLfMDylL+O6ygH8CUApgASAJap6v4gUAfgzgAp0j+y6VlWPpNqWQhAPutNDDSsqqd8L03a4F0+2BQDZId4zLdrK5ztl5aUowBB3mqfhCC/oGDOI96VbtoGP6xof4Ovv+Mhd6AQAP4M7jdkY5T3wykt4gU5nPEUhToynUxv2u1N6pdN4v8OMna9SbUvxmVRDZoqehn//E6e9rpGnHFs1xTlQ4O63BwD5JaVUyz1aTbWaDHfPwMvH8x5+r75b5bS3x/m52JO+XPljAP5OVScAmAPgFhGZCOAOAEtVdRyApcn/G4bxJaHX4FfVGlVdnfy9GcBmAMMAXA3gseTDHgPAW8AahnHa8YU+84tIBYBpAFYCGKqqNUD3CwQA/n7IMIzTjj4Hv4jkAngWwG2qyj8wfd5vsYhUikhle0ffP48YhnFq6VPwi0gmugP/cVV9Lmk+JCJlSb0MQK3LV1WXqOoMVZ0RyeI3ZgzDSC+9Br+ICICHAWxW1fuOk14CsCj5+yIAL5785RmGcarotYefiFwIYDmA9cB/5pHuRPfn/qcAjACwB8C3VbUh1baGDCnWa7/l7qlW18irx6Sj2WlvaOrkOwvzHn6tcZ6SmTKNax0HDzrteWMvoD5nVL1JtVVV+6i2vZFnYbWN/90adGuxLJ46RJSn7DSFFgzxlFiAnFed4FVxqfbVVcCPR26KXoKJdndVZZDYASDLPeELAHD0cCvVolklVCvJ59fZri73cxZv4Wnio+QUqDmwD9EoafDXg17z/Kq6AqBdCy/ty04Mwzj9sG/4GYanWPAbhqdY8BuGp1jwG4anWPAbhqekdVxXdjhLzxxW7l5IhKei4kH3GhNBnpPJaqunWqPwRovF2byBZyzhrphLZPHMSns9L3TsSvBUWSLBEzE5KdJemQl3Ki1y7gjq07i2jmqDSvi3MsORcVSTA1VO+36S1gKAjE6+r2iAp/PyQjxF2E4SVdJVQH1+cTFPRz64g69/b9NUql0zl3/BrWHVTqd9Xa3ze3MAAG13pwG3792Lto4OG9dlGAbHgt8wPMWC3zA8xYLfMDzFgt8wPMWC3zA8Jb2z+kIZCI8c7NS0nhcERmMkxRYfTn2GlfB+I/VNfFbfkIsWUG1KyF3VV7NnDfVZ0cmrBLPbeZp17KG9VKvK5XP3utSdpmrfyJuWjh3KKyrjncOoNm0O/9uqt7mbk+bUvEN9NrfydF6wYjLVWte4G7wCQEaXO9UaLuigPvfumUO1yVmHqTYkwddf3nmAaocGucMwp4E/Zx0F7tSh8N18DrvyG4anWPAbhqdY8BuGp1jwG4anWPAbhqek9W5/cQT4zgT36031Ib6U16qGOu1zIrwQ5IyvnE+12Cp+xza+ht8Flgr3rdR9bbx3W2FtFdVCRXdTbd51PFtR8fESqlUuesBp//H+T6jP5pa3qbZuB799/G49L2SZ07XWaW+JVFCf8Mf7qZZ/Oy9M+uFVXPvd/37Faf/l8/dQn7Wvf0S1/au2Ua3xHd7fL3/BRVTL2LnJaR9cysd1rdvk7msZi/X9em5XfsPwFAt+w/AUC37D8BQLfsPwFAt+w/AUC37D8JReU30iUg7gTwBK0T2ua4mq3i8idwP4IYBjDeDuVNVXU20rEijG2Tk3OrXRs/5E/T7Y5u7Vd8PfzqQ+27t42ujAVt6jbfnYa6j2lUF7nPbQJl5oM/xcPnJpd+VIqm07q51vczvv/de8ZJ3TvmU6L945f4y7CAcA6gNVVLvokiKq7X3M3QsxEOa97Kacz1Omy4p4ejZTeQ+/wrD77/7pvz5Nfa4b832qzcyr4esYPp5qJbqRai3qTqdGB/PtTT1zmdN+pIGf2z3pS54/BuDvVHW1iOQBWCUixwbQ/UZV/0+f92YYxmlDX2b11QCoSf7eLCKbAfA6T8MwvhR8oc/8IlIBYBq6J/QCwK0isk5EHhGRQSd5bYZhnEL6HPwikgvgWQC3qepRAA8AGANgKrrfGdxL/BaLSKWIVDa28c+/hmGklz4Fv4hkojvwH1fV5wBAVQ+palxVEwD+AGCWy1dVl6jqDFWdUZjNb4wZhpFeeg1+EREADwPYrKr3HWcvO+5hCwFsOPnLMwzjVNGXu/0XALgBwHoROVaqdSeA60VkKgAFUAXgR71tSDQD4Zg7raT7+biucI47ffXH5ddTn4VzeErprLG8+qqmlI9Iatrvri7MCvLqq8Gjyqi2r5qn30aOiFEtFBlDtRFZ7zvtbbfzKrZ1z/MeeLGjvE9ibRXvhdgWdx/jcCFPb2ZU76Zaac55VCto5O8oSxe4x3IVPsHTisP/+lmqvbmfP9faxtN5y9r4aLlBMfe5Gm/h19M1R91/czTBz6me9OVu/wrAOfAsZU7fMIzTG/uGn2F4igW/YXiKBb9heIoFv2F4igW/YXhKWht4ZuQHUbzAnaKoC51J/caPdY9q2ryFJxxeeIOn7CI6hGoVy39Gtdpzy532qm38MHbt56mXnDBPb84p4Omr2BzuJ1nu1Na7P/hH6lN/Nk9HxhN8XzUfV1Jt5z73uLHAFl6tGMjnFWmJg7ycJJjJ/ebud6eWa2/izV8f3F1NtSuuqqDa6mXuppoAMHItP1ffr5/mtGcX8u2Fu9zHUfgEuM9hV37D8BQLfsPwFAt+w/AUC37D8BQLfsPwFAt+w/CUtKb6jkoCb4bdKYozorOpX0TcqajpDf9MfWrKf0C1qyteo9r9T/O0V8s79U57UcVo6pMR5TP3MvV5qmU1u9ObAFCXn6Jh5Y/+1mmfH1pOfaZeNIpqrcuepNo9TzdQrXz65U57aUmE+kTaePqt69MzqJYzmafmEvnfcNpvv91doQkAwx56g2rayJ/PmHxKtY4K9/EAgK/NdKdns7PdKUAAiMfanPYDv/1X6tMTu/IbhqdY8BuGp1jwG4anWPAbhqdY8BuGp1jwG4anpDXVl4gH0Xo4z6kVlvJypJnfneO0zw/zSsD/tfZtqr28m8+mO2fmAqrlZrsr9MJ5PH2VGeMVZ4EEbzL6xiZeTXdR8DtUm9e12mm/d+966vP6k2upFovzuYDnzuVNNXOz3cekJDdFRWKANwQNFv2Fam9t5hV/xd+71GnfdOevqE9VhJ+LsRae6otGCqmWW8Rn2uQVu2ceRpRfmzOL3HP8MkLZ1KcnduU3DE+x4DcMT7HgNwxPseA3DE+x4DcMT+n1br+IZAF4D0A4+fhnVPUuERkF4EkARQBWA7hBVTtTbSvU1YIRdR84tbc38eKY22a7i0t+vYePXCrN5xOB28Dv5mZk8bvbhUPdxSCRdn4HuGXoBKpNymmk2u7qFKOfwIuWJj3we6c9EOLjogIdKYqPsnOoVpCfYsRalvtuf2cTP76TJvExahubMqlWnOnuFwgADau3Oe1PpZhqlZMiIxGK8h6Emp3ijn4W32GiyX38MybxYqaWV9wj7BLNfH096cuVPwrgElU9B93juK8QkTkA7gHwG1UdB+AIgJv7vFfDMAacXoNfuzl2Gc1M/lMAlwB4Jml/DMA1p2SFhmGcEvr0mV9EgskJvbUA3gSwE0Cjqh4bJbsPAP+mhWEYpx19Cn5VjavqVADDAcwC4Pog6/wgLSKLRaRSRCqbW/hnOsMw0ssXutuvqo0A3gEwB0ChiBy7YTgcwAHis0RVZ6jqjLxcfvPIMIz00mvwi8hgESlM/h4BsADAZgDLAHwr+bBFAF48VYs0DOPk05fCnjIAj4lIEN0vFk+p6ssisgnAkyLyDwDWAHi4tw0FIMgVdzFLaV4V9fvVfe5ecR0NPH2SP4KnASMxKiGH19qg67A7pdQxdBb1mZq9m2qvbuJjw8Zn8gKNvMDvqPaPh+uc9rZO/jofSpEGLIlwrTTX3UcOAPbWuAuaJl02jvo0LH2HalUjeM+9sih/QsM7HnXaW1P04qs9wk+CtqYOqpVM5enqSAcfH1eb7z4mU+Lc54MznG+0EQ/x/o496TX4VXUdgM91ElTVXej+/G8YxpcQ+4afYXiKBb9heIoFv2F4igW/YXiKBb9heIqo8gq3k74zkToAx2YrlQBwz79KL7aOz2Lr+CxftnWMVNXBfdlgWoP/MzsWqVTVGQOyc1uHrcPWYW/7DcNXLPgNw1MGMviXDOC+j8fW8VlsHZ/l/9t1DNhnfsMwBhZ7228YnjIgwS8iV4jIVhHZISJ3DMQakuuoEpH1IrJWRCrTuN9HRKRWRDYcZysSkTdFZHvyJ+8GeWrXcbeI7E8ek7UicmUa1lEuIstEZLOIbBSR/5q0p/WYpFhHWo+JiGSJyMci8mlyHb9M2keJyMrk8fiziPBuqH1BVdP6D0AQ3W3ARgMIAfgUwMR0ryO5lioAJQOw37kApgPYcJztnwDckfz9DgD3DNA67gbw0zQfjzIA05O/5wHYBmBiuo9JinWk9ZgAEAC5yd8zAaxEdwOdpwBcl7Q/COBv+rOfgbjyzwKwQ1V3aXer7ycBXD0A6xgwVPU9AD2bFFyN7kaoQJoaopJ1pB1VrVHV1cmcES0OAAAB0UlEQVTfm9HdLGYY0nxMUqwjrWg3p7xp7kAE/zAAx3fFGMjmnwrgDRFZJSKLB2gNxxiqqjVA90kIgHf6OPXcKiLrkh8LTvnHj+MRkQp0949YiQE8Jj3WAaT5mKSjae5ABL84bAOVcrhAVacD+CqAW0Rk7gCt43TiAQBj0D2joQbAvenasYjkAngWwG2qylvtpH8daT8m2o+muX1lIIJ/H4Dy4/5Pm3+ealT1QPJnLYDnMbCdiQ6JSBkAJH/yHk6nEFU9lDzxEgD+gDQdExHJRHfAPa6qzyXNaT8mrnUM1DFJ7vsLN83tKwMR/J8AGJe8cxkCcB2Al9K9CBHJEZG8Y78DuAzAhtRep5SX0N0IFRjAhqjHgi3JQqThmIiIoLsH5GZVve84Ka3HhK0j3cckbU1z03UHs8fdzCvRfSd1J4BfDNAaRqM70/ApgI3pXAeAJ9D99rEL3e+EbgZQDGApgO3Jn0UDtI5/A7AewDp0B19ZGtZxIbrfwq4DsDb578p0H5MU60jrMQEwBd1Ncdeh+4Xmvx93zn4MYAeApwGE+7Mf+4afYXiKfcPPMDzFgt8wPMWC3zA8xYLfMDzFgt8wPMWC3zA8xYLfMDzFgt8wPOX/AiEPtokkBmqfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x292ef9139e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model 4\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "model_name = \"model_4\"\n",
    "init = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               2000,                # 2000 steps\n",
    "                                               0.95,                 # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=32,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "         # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu1 = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            relu1,                           # Input data\n",
    "            filters=32,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu2 = tf.nn.relu(bn2, name='relu2')\n",
    "   \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            relu2,                       # Input\n",
    "            pool_size=(2, 2),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu3 = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            relu3,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        norm4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu4 = tf.nn.relu(norm4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 2 (2x2, stride: 2) - TUNED\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            relu4,                       # input\n",
    "            pool_size=(2, 2),            # pool size 2x2\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "        # dropout at 50%\n",
    "        # flat_output = tf.layers.dropout(flat_output, rate=0.5, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            512,                         # 256 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        # dropout at 20%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.250, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + (alpha * tf.losses.get_regularization_loss())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 7\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = False\n",
    "model_name = \"model_7\"\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               3200,                # 3200 steps\n",
    "                                               0.85,                # 0.85 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-5\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv1'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "\n",
    "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=0, training=training)\n",
    "    \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv1_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
    "             \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool2, rate=0.1, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=96,                  # 64 filters\n",
    "            kernel_size=(4, 4),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=2, training=training)\n",
    "        \n",
    "    with tf.name_scope('pool3') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            conv3_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool3 = tf.layers.dropout(pool3, rate=0.1, seed=1, training=training)\n",
    "        \n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            pool3,                       # Input\n",
    "            filters=96,                  # 64 filters\n",
    "            kernel_size=(4, 4),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
    "    \n",
    "    # Max pooling layer 2 \n",
    "    pool4 = tf.layers.max_pooling2d(\n",
    "        conv4_bn_relu,                       # input\n",
    "        pool_size=(3, 3),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool2'\n",
    "    )\n",
    "\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        # Convolutional layer 5\n",
    "        conv5= tf.layers.conv2d(\n",
    "            pool4,                       # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv5'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn5 = tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn5'\n",
    "        )\n",
    "        \n",
    "        # activation\n",
    "        conv5_bn_relu = tf.nn.relu(bn5, name='relu5')\n",
    "\n",
    "        # try dropout here\n",
    "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
    "\n",
    "    # Max pooling layer 2 \n",
    "    pool5 = tf.layers.max_pooling2d(\n",
    "        conv5_bn_relu,                       # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool5'\n",
    "    )\n",
    "        \n",
    "    with tf.name_scope('conv6') as scope:\n",
    "        # Convolutional layer 6\n",
    "        conv6= tf.layers.conv2d(\n",
    "            pool5,               # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=4),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv6'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn6 = tf.layers.batch_normalization(\n",
    "            conv6,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn6'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
    "    \n",
    "    # Max pooling layer 3\n",
    "    pool6 = tf.layers.max_pooling2d(\n",
    "        conv6_bn_relu,               # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool6'\n",
    "    )\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool6)\n",
    "\n",
    "        ## TRY THIS\n",
    "        # dropout at 20%\n",
    "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            1024,                        # 1024 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "        \n",
    "        # dropout at 10%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.10, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + (alpha * tf.losses.get_regularization_loss())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model 10\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "model_name = \"model_10\"\n",
    "init = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               3200,                # 2400 steps\n",
    "                                               0.9,                 # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "         # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu1 = tf.nn.relu(bn1, name='relu1')\n",
    "\n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            relu1,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "        \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu2 = tf.nn.relu(bn2, name='relu2')\n",
    "   \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            relu2,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool2,                       # Input\n",
    "            filters=96,                  # 96 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu3 = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            relu3,                       # Input\n",
    "            filters=96,                  # 96 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        norm4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu4 = tf.nn.relu(norm4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool3') as scope:\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            relu4,                       # input\n",
    "            pool_size=(2, 2),            # pool size 2x2\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool3 = tf.layers.dropout(pool3, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool3)\n",
    "\n",
    "        # dropout at 12.50%\n",
    "        #flat_output = tf.layers.dropout(flat_output, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            784,                         # 784 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.1250, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        # Fully connected layer\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1,                         # input\n",
    "            392,                         # 392 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc2'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc2 = tf.layers.dropout(fc2, rate=0.1250, seed=1, training=training)        \n",
    "        \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + (alpha * tf.losses.get_regularization_loss())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 1\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = True\n",
    "crop = False\n",
    "model_name = \"model_1a\"\n",
    "# model 1 has default learning rate, model 1a has specified learning rate\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    #learning_rate = tf.train.exponential_decay(0.002,               # start at 0.002\n",
    "    #                                           global_step, \n",
    "    #                                           2000,                # 2000 steps\n",
    "    #                                           0.9,                 # 0.90 increment\n",
    "    #                                           staircase=True)\n",
    "    \n",
    "    learning_rate = create_learning_rate(global_step, start=0.002, steps=2000, decay=0.9, staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    # Convolutional layer 1     \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       \n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv1'                 \n",
    "        )\n",
    "\n",
    "         # batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # relu activation\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    # Max pooling layer 1\n",
    "    with tf.name_scope('pool1') as scope:\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv1_bn_relu,               # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # dropout at 0.125\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "       \n",
    "    # Convolutional layer 2\n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       \n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv2'           \n",
    "        )\n",
    "\n",
    "        # batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # relu activation\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    # Max pooling layer 2 (3x3, stride: 2)\n",
    "    with tf.name_scope('pool2') as scope:\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,               # input\n",
    "            pool_size=(3, 3),            # pool size 3x3\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # dropout at 0.125\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    # Flatten output\n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        flat_output = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "        # dropout at 25%\n",
    "        flat_output = tf.layers.dropout(flat_output, rate=0.25, seed=1, training=training)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            384,                         # 384 hidden units\n",
    "            activation=None,       \n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='fc1'\n",
    "\n",
    "        )\n",
    "\n",
    "        # batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            fc1,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # relu activation\n",
    "        fc1_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "        # dropout at 30%\n",
    "        # fc1 = tf.layers.dropout(fc1, rate=0.30, seed=1, training=training)\n",
    "    \n",
    "    # Fully connected layer 2\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1_bn_relu,                 # input\n",
    "            192,                         # 192 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        # dropout at 25%\n",
    "        fc2 = tf.layers.dropout(fc2, rate=0.25, seed=1, training=training)\n",
    "    \n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2,                         # input\n",
    "        num_classes,                 # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels = tf.get_variable('kernel')\n",
    "    \n",
    "    with tf.variable_scope('conv2', reuse=True):\n",
    "        conv_kernels2 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + (alpha * tf.losses.get_regularization_loss())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # add this so that the batch norm gets \n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 2\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = False\n",
    "model_name = \"model_2\"\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.004,                 # start at 0.005\n",
    "                                               global_step, \n",
    "                                               20000,                 # decay by 0.3 in 25 epochs\n",
    "                                               0.2,                   # 0.3 decrease\n",
    "                                               staircase=False)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv1_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "        \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool2,                       # Input\n",
    "            filters=128,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "        \n",
    "    with tf.name_scope('pool3') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            conv3_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool3 = tf.layers.dropout(pool3, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            pool3,                       # Input\n",
    "            filters=128,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool4') as scope:\n",
    "         # Max pooling layer 2 (2x2, stride: 2) - TUNED\n",
    "        pool4 = tf.layers.max_pooling2d(\n",
    "            conv4_bn_relu,                       # input\n",
    "            pool_size=(3, 3),            # pool size 3x3\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name=\"pool4\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # try dropout here\n",
    "        pool4 = tf.layers.dropout(pool4, rate=0.125, seed=4, training=training)\n",
    "    \n",
    "    # Flatten output\n",
    "    flat_output = tf.contrib.layers.flatten(pool4)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            512,                         # 256 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=5),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "\n",
    "        # dropout at 25%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.250, seed=6, training=training)\n",
    "    \n",
    "     # Fully connected layer 2\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1,                 # input\n",
    "            256,                         # 192 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        # dropout at 25%\n",
    "        fc2 = tf.layers.dropout(fc2, rate=0.25, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=7),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + (alpha * tf.losses.get_regularization_loss())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model 9.2r\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "model_name = \"model_9.2r\"\n",
    "init = True\n",
    "crop = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.005,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               3200,                # 3200 steps\n",
    "                                               0.85,                 # 0.9 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    # regularization alpha\n",
    "    alpha = 0.01\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "         # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu1 = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            relu1,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu2 = tf.nn.relu(bn2, name='relu2')\n",
    "   \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            relu2,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=96,                  # 96 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu3 = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            relu3,                       # Input\n",
    "            filters=96,                  # 96 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        norm4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu4 = tf.nn.relu(norm4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            relu4,                       # input\n",
    "            pool_size=(2, 2),            # pool size 2x2\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "        # dropout at 12.50%\n",
    "        #flat_output = tf.layers.dropout(flat_output, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            512,                         # 512 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.1250, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        # Fully connected layer\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1,                         # input\n",
    "            256,                         # 256 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc2'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc2 = tf.layers.dropout(fc2, rate=0.1250, seed=1, training=training)        \n",
    "        \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + (alpha * tf.losses.get_regularization_loss())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model 12\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "model_name = \"model_12\"\n",
    "init = True\n",
    "crop = True\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 24, 24, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               3200,                # 2400 steps\n",
    "                                               0.85,                 # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "         # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu1 = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            relu1,                           # Input data\n",
    "            filters=32,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu2 = tf.nn.relu(bn2, name='relu2')\n",
    "   \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            relu2,                       # Input\n",
    "            pool_size=(2, 2),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu3 = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            relu3,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        norm4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu4 = tf.nn.relu(norm4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 2 (2x2, stride: 2) - TUNED\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            relu4,                       # input\n",
    "            pool_size=(2, 2),            # pool size 2x2\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "        # dropout at 50%\n",
    "        # flat_output = tf.layers.dropout(flat_output, rate=0.5, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            512,                         # 512 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.1250, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        # Fully connected layer\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1,                         # input\n",
    "            256,                         # 256 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc2'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc2 = tf.layers.dropout(fc2, rate=0.1250, seed=1, training=training)        \n",
    "        \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + (alpha * tf.losses.get_regularization_loss())\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
