{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "\n",
    "X_test = test_data[b'data']\n",
    "X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:])) / 255.0\n",
    "X_test = X_test.reshape(-1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = X_test[...,::-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2094497d358>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20943920128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 2\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = False\n",
    "model_name = \"model_2\"\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.004,                 # start at 0.005\n",
    "                                               global_step, \n",
    "                                               20000,                 # decay by 0.3 in 25 epochs\n",
    "                                               0.2,                   # 0.3 decrease\n",
    "                                               staircase=False)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv1_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "        \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool2,                       # Input\n",
    "            filters=128,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "        \n",
    "    with tf.name_scope('pool3') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            conv3_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool3 = tf.layers.dropout(pool3, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            pool3,                       # Input\n",
    "            filters=128,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool4') as scope:\n",
    "         # Max pooling layer 2 (2x2, stride: 2) - TUNED\n",
    "        pool4 = tf.layers.max_pooling2d(\n",
    "            conv4_bn_relu,                       # input\n",
    "            pool_size=(3, 3),            # pool size 3x3\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name=\"pool4\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # try dropout here\n",
    "        pool4 = tf.layers.dropout(pool4, rate=0.125, seed=4, training=training)\n",
    "    \n",
    "    # Flatten output\n",
    "    flat_output = tf.contrib.layers.flatten(pool4)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            512,                         # 256 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=5),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "\n",
    "        # dropout at 25%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.250, seed=6, training=training)\n",
    "    \n",
    "     # Fully connected layer 2\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1,                 # input\n",
    "            256,                         # 192 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        # dropout at 25%\n",
    "        fc2 = tf.layers.dropout(fc2, rate=0.25, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=7),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model 4\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "model_name = \"model_4\"\n",
    "init = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               2000,                # 2000 steps\n",
    "                                               0.95,                 # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=32,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "         # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu1 = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            relu1,                           # Input data\n",
    "            filters=32,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu2 = tf.nn.relu(bn2, name='relu2')\n",
    "   \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            relu2,                       # Input\n",
    "            pool_size=(2, 2),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu3 = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            relu3,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        norm4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu4 = tf.nn.relu(norm4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 2 (2x2, stride: 2) - TUNED\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            relu4,                       # input\n",
    "            pool_size=(2, 2),            # pool size 2x2\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "        # dropout at 50%\n",
    "        # flat_output = tf.layers.dropout(flat_output, rate=0.5, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            512,                         # 256 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        # dropout at 20%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.250, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 7\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = False\n",
    "model_name = \"model_7\"\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               2000,                # 2400 steps\n",
    "                                               0.95,                # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-5\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv1'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "\n",
    "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=0, training=training)\n",
    "    \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv1_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
    "             \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool2, rate=0.1, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=96,                  # 64 filters\n",
    "            kernel_size=(4, 4),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=2, training=training)\n",
    "        \n",
    "    with tf.name_scope('pool3') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            conv3_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool3 = tf.layers.dropout(pool3, rate=0.1, seed=1, training=training)\n",
    "        \n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            pool3,                       # Input\n",
    "            filters=96,                  # 64 filters\n",
    "            kernel_size=(4, 4),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
    "    \n",
    "    # Max pooling layer 2 \n",
    "    pool4 = tf.layers.max_pooling2d(\n",
    "        conv4_bn_relu,                       # input\n",
    "        pool_size=(3, 3),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool2'\n",
    "    )\n",
    "\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        # Convolutional layer 5\n",
    "        conv5= tf.layers.conv2d(\n",
    "            pool4,                       # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv5'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn5 = tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn5'\n",
    "        )\n",
    "        \n",
    "        # activation\n",
    "        conv5_bn_relu = tf.nn.relu(bn5, name='relu5')\n",
    "\n",
    "        # try dropout here\n",
    "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
    "\n",
    "    # Max pooling layer 2 \n",
    "    pool5 = tf.layers.max_pooling2d(\n",
    "        conv5_bn_relu,                       # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool5'\n",
    "    )\n",
    "        \n",
    "    with tf.name_scope('conv6') as scope:\n",
    "        # Convolutional layer 6\n",
    "        conv6= tf.layers.conv2d(\n",
    "            pool5,               # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=4),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv6'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn6 = tf.layers.batch_normalization(\n",
    "            conv6,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn6'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
    "    \n",
    "    # Max pooling layer 3\n",
    "    pool6 = tf.layers.max_pooling2d(\n",
    "        conv6_bn_relu,               # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool6'\n",
    "    )\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool6)\n",
    "\n",
    "        ## TRY THIS\n",
    "        # dropout at 20%\n",
    "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            1024,                        # 1024 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "        \n",
    "        # dropout at 10%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.10, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 3\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = True\n",
    "model_name = \"model_3\"\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.03,               # start at 0.03\n",
    "                                               global_step, \n",
    "                                               500,                # 500 steps\n",
    "                                               0.95,               # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=32,                  # 32 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv1'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "\n",
    "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=0, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv1_bn_relu,                           # Input data\n",
    "            filters=32,                  # 32 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,                       # Input\n",
    "            pool_size=(2, 2),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=2, training=training)\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            conv3_bn_relu,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
    "    \n",
    "    # Max pooling layer 2 \n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        conv4_bn_relu,                       # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool2'\n",
    "    )\n",
    "\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        # Convolutional layer 5\n",
    "        conv5= tf.layers.conv2d(\n",
    "            pool2,                       # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv5'                 # Add name\n",
    "        )\n",
    "\n",
    "        # activation\n",
    "        conv5_bn_relu = tf.nn.relu(conv5, name='relu5')\n",
    "\n",
    "        # try dropout here\n",
    "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
    "\n",
    "    with tf.name_scope('conv6') as scope:\n",
    "        # Convolutional layer 6\n",
    "        conv6= tf.layers.conv2d(\n",
    "            conv5_bn_relu,               # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=4),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv6'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn6 = tf.layers.batch_normalization(\n",
    "            conv6,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn6'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
    "    \n",
    "    # Max pooling layer 3\n",
    "    pool3 = tf.layers.max_pooling2d(\n",
    "        conv6_bn_relu,               # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool3'\n",
    "    )\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool3)\n",
    "\n",
    "        ## TRY THIS\n",
    "        # dropout at 20%\n",
    "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            1024,                        # 1024 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "        \n",
    "        # dropout at 10%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.10, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = optimizer.minimize(mean_ce, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model 10\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "model_name = \"model_10\"\n",
    "init = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               3200,                # 2400 steps\n",
    "                                               0.9,                 # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "         # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu1 = tf.nn.relu(bn1, name='relu1')\n",
    "\n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            relu1,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "        \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                           # Input data\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu2 = tf.nn.relu(bn2, name='relu2')\n",
    "   \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            relu2,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool2,                       # Input\n",
    "            filters=96,                  # 96 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu3 = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            relu3,                       # Input\n",
    "            filters=96,                  # 96 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        norm4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu4 = tf.nn.relu(norm4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool3') as scope:\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            relu4,                       # input\n",
    "            pool_size=(2, 2),            # pool size 2x2\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool3 = tf.layers.dropout(pool3, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool3)\n",
    "\n",
    "        # dropout at 12.50%\n",
    "        #flat_output = tf.layers.dropout(flat_output, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            784,                         # 784 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.1250, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        # Fully connected layer\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1,                         # input\n",
    "            392,                         # 392 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc2'\n",
    "        )\n",
    "        \n",
    "        # dropout at 12.5%\n",
    "        fc2 = tf.layers.dropout(fc2, rate=0.1250, seed=1, training=training)        \n",
    "        \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
