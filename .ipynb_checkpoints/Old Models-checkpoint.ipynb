{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model 4\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "model_name = \"model_4\"\n",
    "init = False\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               2000,                # 2000 steps\n",
    "                                               0.95,                 # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=32,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv1'                  # Add name\n",
    "        )\n",
    "\n",
    "         # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu1 = tf.nn.relu(bn1, name='relu1')\n",
    "    \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            relu1,                           # Input data\n",
    "            filters=32,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu2 = tf.nn.relu(bn2, name='relu2')\n",
    "   \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            relu2,                       # Input\n",
    "            pool_size=(2, 2),            # Pool size: 3x3\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu3 = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            relu3,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 5x5\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.001),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        norm4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        relu4 = tf.nn.relu(norm4, name='relu4')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 2 (2x2, stride: 2) - TUNED\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            relu4,                       # input\n",
    "            pool_size=(2, 2),            # pool size 2x2\n",
    "            strides=(2, 2),              # stride 2\n",
    "            padding='SAME',\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool2 = tf.layers.dropout(pool2, rate=0.125, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "        # dropout at 50%\n",
    "        # flat_output = tf.layers.dropout(flat_output, rate=0.5, seed=1, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            512,                         # 256 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.0001),\n",
    "            name='fc1'\n",
    "        )\n",
    "        \n",
    "        # dropout at 20%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.250, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='logits'\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 7\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = False\n",
    "model_name = \"model_7\"\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.003,               # start at 0.003\n",
    "                                               global_step, \n",
    "                                               2000,                # 2400 steps\n",
    "                                               0.95,                # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-5\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv1'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "\n",
    "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=0, training=training)\n",
    "    \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv1_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
    "             \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,                           # Input data\n",
    "            filters=64,                  # 32 filters\n",
    "            kernel_size=(5, 5),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    with tf.name_scope('pool2') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool2, rate=0.1, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=96,                  # 64 filters\n",
    "            kernel_size=(4, 4),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=2, training=training)\n",
    "        \n",
    "    with tf.name_scope('pool3') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            conv3_bn_relu,                       # Input\n",
    "            pool_size=(3, 3),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool3 = tf.layers.dropout(pool3, rate=0.1, seed=1, training=training)\n",
    "        \n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            pool3,                       # Input\n",
    "            filters=96,                  # 64 filters\n",
    "            kernel_size=(4, 4),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
    "    \n",
    "    # Max pooling layer 2 \n",
    "    pool4 = tf.layers.max_pooling2d(\n",
    "        conv4_bn_relu,                       # input\n",
    "        pool_size=(3, 3),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool2'\n",
    "    )\n",
    "\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        # Convolutional layer 5\n",
    "        conv5= tf.layers.conv2d(\n",
    "            pool4,                       # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv5'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn5 = tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn5'\n",
    "        )\n",
    "        \n",
    "        # activation\n",
    "        conv5_bn_relu = tf.nn.relu(bn5, name='relu5')\n",
    "\n",
    "        # try dropout here\n",
    "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
    "\n",
    "    # Max pooling layer 2 \n",
    "    pool5 = tf.layers.max_pooling2d(\n",
    "        conv5_bn_relu,                       # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool5'\n",
    "    )\n",
    "        \n",
    "    with tf.name_scope('conv6') as scope:\n",
    "        # Convolutional layer 6\n",
    "        conv6= tf.layers.conv2d(\n",
    "            pool5,               # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=4),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv6'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn6 = tf.layers.batch_normalization(\n",
    "            conv6,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=False,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn6'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
    "    \n",
    "    # Max pooling layer 3\n",
    "    pool6 = tf.layers.max_pooling2d(\n",
    "        conv6_bn_relu,               # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool6'\n",
    "    )\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool6)\n",
    "\n",
    "        ## TRY THIS\n",
    "        # dropout at 20%\n",
    "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            1024,                        # 1024 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "        \n",
    "        # dropout at 10%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.10, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MODEL 3\n",
    "# Create new graph\n",
    "graph = tf.Graph()\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = True\n",
    "model_name = \"model_3\"\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Decay the learning rate - \n",
    "    learning_rate = tf.train.exponential_decay(0.03,               # start at 0.03\n",
    "                                               global_step, \n",
    "                                               500,                # 500 steps\n",
    "                                               0.95,               # 0.95 increment\n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Small epsilon value for the BN transform\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        # Convolutional layer 1 \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X,                           # Input data\n",
    "            filters=32,                  # 32 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv1'                 # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
    "\n",
    "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=0, training=training)\n",
    "    \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        # Convolutional layer 2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            conv1_bn_relu,                           # Input data\n",
    "            filters=32,                  # 32 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv2'                  # Add name\n",
    "        )\n",
    "\n",
    "        # try batch normalization\n",
    "        bn2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
    "    \n",
    "    with tf.name_scope('pool1') as scope:\n",
    "         # Max pooling layer 1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv2_bn_relu,                       # Input\n",
    "            pool_size=(2, 2),            # Pool size: 2x2\n",
    "            strides=(2, 2),              # Stride: 2\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # try dropout here\n",
    "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
    "\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        # Convolutional layer 3\n",
    "        conv3= tf.layers.conv2d(\n",
    "            pool1,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv3'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
    "\n",
    "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=2, training=training)\n",
    "\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        # Convolutional layer 4\n",
    "        conv4= tf.layers.conv2d(\n",
    "            conv3_bn_relu,                       # Input\n",
    "            filters=64,                  # 64 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv4'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn4 = tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn4'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
    "    \n",
    "    # Max pooling layer 2 \n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        conv4_bn_relu,                       # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool2'\n",
    "    )\n",
    "\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        # Convolutional layer 5\n",
    "        conv5= tf.layers.conv2d(\n",
    "            pool2,                       # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,       # ReLU\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv5'                 # Add name\n",
    "        )\n",
    "\n",
    "        # activation\n",
    "        conv5_bn_relu = tf.nn.relu(conv5, name='relu5')\n",
    "\n",
    "        # try dropout here\n",
    "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
    "\n",
    "    with tf.name_scope('conv6') as scope:\n",
    "        # Convolutional layer 6\n",
    "        conv6= tf.layers.conv2d(\n",
    "            conv5_bn_relu,               # Input\n",
    "            filters=128,                 # 128 filters\n",
    "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
    "            strides=(1, 1),              # Stride: 1\n",
    "            padding='SAME',              # \"same\" padding\n",
    "            activation=None,             # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=4),    # Small standard deviation\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name='conv6'                 # Add name\n",
    "        )\n",
    "\n",
    "        bn6 = tf.layers.batch_normalization(\n",
    "            conv6,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn6'\n",
    "        )\n",
    "\n",
    "        #apply relu\n",
    "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
    "    \n",
    "    # Max pooling layer 3\n",
    "    pool3 = tf.layers.max_pooling2d(\n",
    "        conv6_bn_relu,               # input\n",
    "        pool_size=(2, 2),            # pool size 2x2\n",
    "        strides=(2, 2),              # stride 2\n",
    "        padding='SAME',\n",
    "        name='pool3'\n",
    "    )\n",
    "    \n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        # Flatten output\n",
    "        flat_output = tf.contrib.layers.flatten(pool3)\n",
    "\n",
    "        ## TRY THIS\n",
    "        # dropout at 20%\n",
    "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
    "    \n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,                 # input\n",
    "            1024,                        # 1024 hidden units\n",
    "            activation=tf.nn.relu,       # ReLU\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "        \n",
    "        # dropout at 10%\n",
    "        fc1 = tf.layers.dropout(fc1, rate=0.10, seed=1, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc1,                         # input\n",
    "        num_classes,                           # One output unit per category\n",
    "        activation=None,             # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "    \n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "    \n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize cross-entropy\n",
    "    train_op = optimizer.minimize(mean_ce, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
