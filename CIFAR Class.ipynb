{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr (16560, 32, 32, 3)\n",
      "X_cv (1440, 32, 32, 3)\n",
      "X_te (2000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "with np.load(os.path.join('data','cifar10-20k.npz'), allow_pickle=False) as npz_file:\n",
    "    data = dict(npz_file.items())\n",
    "    \n",
    "y_full = data['labels']\n",
    "names = data['names']\n",
    "X_full = data['data'].reshape(-1, 32, 32, 3)\n",
    "\n",
    "# normalize the data\n",
    "X_full = X_full / 255\n",
    "\n",
    "# set number of classes\n",
    "num_classes = names.shape[0]\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr_full, X_te, y_tr_full, y_te = train_test_split(X_full, y_full, test_size=0.1, random_state=1)\n",
    "\n",
    "# Split data again into training and cv\n",
    "X_tr, X_cv, y_tr, y_cv = train_test_split(X_tr_full, y_tr_full, test_size=0.08, random_state=1)\n",
    "\n",
    "print(\"X_tr\", X_tr.shape)\n",
    "print(\"X_cv\", X_cv.shape)\n",
    "print(\"X_te\", X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "\n",
    "    #To build the graph when instantiated\n",
    "    def __init__(self, num_classes=10, model_name='model'):\n",
    "        # Initialize variables\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.graph = tf.Graph()\n",
    "        self.valid_acc_values = []\n",
    "        self.valid_cost_values = []\n",
    "        self.train_acc_values = []\n",
    "        self.train_cost_values = []\n",
    "        self.init = True\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            # placeholders\n",
    "            self.X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "            self.y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "            self.training = tf.placeholder(dtype=tf.bool)\n",
    "            \n",
    "            # create global step for decaying learning rate\n",
    "            self.global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "            # Decay the learning rate\n",
    "            self.learning_rate = tf.train.exponential_decay(0.001,self.global_step, 2000,0.95,staircase=True)\n",
    "            \n",
    "            # Convolutional layer 1 \n",
    "            self.conv1 = tf.layers.conv2d(\n",
    "                self.X,                           # Input data\n",
    "                filters=64,                  # 64 filters\n",
    "                kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "                strides=(1, 1),              # Stride: 2\n",
    "                padding='SAME',              # \"same\" padding\n",
    "                activation=tf.nn.relu,       # ReLU\n",
    "                kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0), # Small standard deviation\n",
    "                name='conv1'                  # Add name\n",
    "                )\n",
    "    \n",
    "            # Max pooling layer 1\n",
    "            self.pool1 = tf.layers.max_pooling2d(\n",
    "                self.conv1,                       # Input\n",
    "                pool_size=(3, 3),            # Pool size: 3x3\n",
    "                strides=(2, 2),              # Stride: 2\n",
    "                padding='SAME',              # \"same\" padding\n",
    "                name='pool1'\n",
    "            )\n",
    "            \n",
    "            self.norm1 = tf.nn.lrn(self.pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "            \n",
    "            # Convolutional layer 2\n",
    "            self.conv2 = tf.layers.conv2d(\n",
    "                self.norm1,                       # Input\n",
    "                filters=64,                  # 64 filters\n",
    "                kernel_size=(5, 5),          # Kernel size: 5x5\n",
    "                strides=(1, 1),              # Stride: 1\n",
    "                padding='SAME',              # \"same\" padding\n",
    "                activation=tf.nn.relu,       # ReLU\n",
    "                kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=0),    # Small standard deviation\n",
    "                name='conv2'                 # Add name\n",
    "            )\n",
    "    \n",
    "            self.norm2 = tf.nn.lrn(self.conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    \n",
    "            # Max pooling layer 2 (2x2, stride: 2) - TUNED\n",
    "            self.pool2 = tf.layers.max_pooling2d(\n",
    "                self.norm2,                       # input\n",
    "                pool_size=(3, 3),            # pool size 2x2\n",
    "                strides=(2, 2),              # stride 2\n",
    "                padding='SAME'\n",
    "            )\n",
    "    \n",
    "            # try dropout here\n",
    "            self.pool2 = tf.layers.dropout(self.pool2, rate=0.25, seed=1, training=self.training)\n",
    "    \n",
    "            # Flatten output\n",
    "            self.flat_output = tf.contrib.layers.flatten(self.pool2)\n",
    "    \n",
    "            # dropout at 50%\n",
    "            self.flat_output = tf.layers.dropout(self.flat_output, rate=0.5, seed=1, training=self.training)\n",
    "    \n",
    "            # Fully connected layer\n",
    "            self.fc1 = tf.layers.dense(\n",
    "                self.flat_output,                 # input\n",
    "                384,                         # 256 hidden units\n",
    "                activation=tf.nn.relu,       # ReLU\n",
    "                kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "                bias_initializer=tf.zeros_initializer()\n",
    "            )\n",
    "    \n",
    "            self.fc2 = tf.layers.dense(\n",
    "                self.fc1,                 # input\n",
    "                192,                         # 256 hidden units\n",
    "                activation=tf.nn.relu,       # ReLU\n",
    "                kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "                bias_initializer=tf.zeros_initializer()\n",
    "            )\n",
    "        \n",
    "            self.logits = tf.layers.dense(\n",
    "                self.fc2,                         # input\n",
    "                self.num_classes,                           # One output unit per category\n",
    "                activation=None,             # No activation function\n",
    "                kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "                bias_initializer=tf.zeros_initializer()\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # Mean cross-entropy\n",
    "            self.mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits))\n",
    "        \n",
    "            # Adam optimizer\n",
    "            self.gd = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "    \n",
    "            # Minimize cross-entropy\n",
    "            self.train_op = self.gd.minimize(self.mean_ce, global_step=self.global_step)\n",
    "\n",
    "            # Compute predictions and accuracy\n",
    "            self.predictions = tf.argmax(self.logits, axis=1, output_type=tf.int32)\n",
    "            self.is_correct = tf.equal(self.y, self.predictions)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.is_correct, dtype=tf.float32))\n",
    "    \n",
    "    ## Parameters - \n",
    "    # X - features\n",
    "    # y - labels\n",
    "    # batch_size\n",
    "    # epochs\n",
    "    # use_gpu\n",
    "    # print_metrics - whether to print or plot results\n",
    "    # init - whether to initialize a new model or restore a saved model\n",
    "    # X_cv, y_cv - cross validation data (optional)    \n",
    "    # print_every - how often to print out metrics, in epochs\n",
    "    def train(self, X_tr, y_tr, batch_size=64, epochs=20, use_gpu=False, print_metrics=True, init=True, X_cv=None, y_cv=None, print_every=5):\n",
    "        self.init = init\n",
    "        \n",
    "        if use_gpu:\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allocator_type = 'BFC'\n",
    "            config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "        else:\n",
    "            config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "        \n",
    "        with tf.Session(graph=self.graph, config=config) as sess:\n",
    "            if not print_metrics:\n",
    "                # create a plot to be updated as model is trained\n",
    "                f, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "    \n",
    "            # create the saver\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "             # If the model is new initialize variables, else restore the session\n",
    "            if self.init:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            else:\n",
    "                saver.restore(sess, './model/'+self.model_name+'.ckpt')\n",
    "                \n",
    "            # Set seed\n",
    "            np.random.seed(0)\n",
    "            \n",
    "            # Train over multiple epochs\n",
    "            for epoch in range(epochs):\n",
    "                # Accuracy values (train) after each batch\n",
    "                batch_acc = []\n",
    "                batch_cost = []\n",
    "                \n",
    "                for X_batch, y_batch in self.get_batches(X_tr, y_tr, batch_size):\n",
    "                    # Run training and evaluate accuracy\n",
    "                    _, acc_value, cost_value = sess.run([self.train_op, self.accuracy, self.mean_ce], feed_dict={\n",
    "                        self.X: X_batch,\n",
    "                        self.y: y_batch,\n",
    "                        self.training: True\n",
    "                    })\n",
    "\n",
    "                    # Save accuracy (current batch)\n",
    "                    batch_acc.append(acc_value)\n",
    "                    batch_cost.append(cost_value)\n",
    "                    \n",
    "                if (X_cv != None) and (y_cv != None):\n",
    "                    valid_acc, valid_cost = self.evaluate(X_cv, y_cv)\n",
    "\n",
    "                if print_metrics:\n",
    "                    # Print progress every fifth epoch to keep output to reasonable amount\n",
    "                    if(epoch % print_every == 0):\n",
    "                        print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
    "                            epoch, step, valid_acc, np.mean(batch_acc), valid_cost, lr\n",
    "                        ))\n",
    "                    else:\n",
    "                        self.plot_metrics()\n",
    "\n",
    "                # save checkpoint every 10th epoch except the first\n",
    "                if((epoch != 0) & (epoch % 10 == 0)):\n",
    "                    self.save_model(sess, model_name)\n",
    "                \n",
    "                # save after the model is trained\n",
    "                self.save_model(sess, model_name)\n",
    "                \n",
    "                if (X_cv != None):\n",
    "                    return self.valid_acc_values\n",
    "                else:\n",
    "                    return self.train_acc_values\n",
    "            \n",
    "    def evaluate(self, X_cv, y_cv):\n",
    "        # Evaluate validation accuracy\n",
    "        valid_acc, valid_cost, lr = sess.run([self.accuracy, self.mean_ce, self.learning_rate], feed_dict={\n",
    "            self.X: X_cv,\n",
    "            self.y: y_cv,\n",
    "            self.training: False\n",
    "        })\n",
    "        self.valid_acc_values.append(valid_acc)\n",
    "        self.valid_cost_values.append(valid_cost)\n",
    "        self.train_acc_values.append(np.mean(batch_acc))\n",
    "        self.train_cost_values.append(np.mean(batch_cost))\n",
    "        \n",
    "        return valid_acc, valid_cost\n",
    "        \n",
    "    def predict(self, X_test, y):\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            saver.restore(sess, './model/'+self.model_name+'.ckpt')\n",
    "\n",
    "            test_labels = sess.run([predictions], feed_dict = \n",
    "                {\n",
    "                    self.X: X_test, \n",
    "                    self.training:False\n",
    "                })\n",
    "            \n",
    "            return test_labels\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        # draw a plot\n",
    "        ax[0].cla()\n",
    "        ax[0].plot(self.valid_acc_values, color=\"red\", label=\"Validation\")\n",
    "        ax[0].plot(self.train_acc_values, color=\"blue\", label=\"Training\")\n",
    "        ax[0].set_title('Validation accuracy: {:.3f} (mean last 3)'.format(np.mean(self.valid_acc_values[-3:])))\n",
    "        ax[0].set_xlabel('epoch')\n",
    "        ax[0].set_ylabel('accuracy')\n",
    "        \n",
    "        ax[1].cla()\n",
    "        ax[1].plot(self.valid_cost_values, color=\"red\", label=\"Validation\")\n",
    "        ax[1].plot(self.train_cost_values, color=\"blue\", label=\"Training\")\n",
    "        ax[1].set_title('Validation cost: {:.3f} (mean last 3)'.format(np.mean(self.valid_cost_values[-3:])))\n",
    "        ax[1].set_xlabel('epoch')\n",
    "        ax[1].set_ylabel('cost')\n",
    "        ax[1].legend()\n",
    "        ax[0].legend()\n",
    "\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "    def save_model(sess):\n",
    "        print(\"Saving checkpoint\")\n",
    "        # save the model\n",
    "        save_path = saver.save(sess, './model/'+self.model_name+'.ckpt')\n",
    "\n",
    "        # Now that model is saved set init to false so we don't need to constantly retrain it\n",
    "        self.init = False\n",
    "        \n",
    "    def get_batches(self, X, y, batch_size):\n",
    "        # Shuffle X,y\n",
    "        shuffled_idx = np.arange(len(y))\n",
    "        np.random.shuffle(shuffled_idx)\n",
    "\n",
    "        # Enumerate indexes by steps of batch_size\n",
    "        for i in range(0, len(y), batch_size):\n",
    "            # Batch indexes\n",
    "            batch_idx = shuffled_idx[i:i+batch_size]\n",
    "            yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2e86a42e2cf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-cf99c9083d10>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_tr, y_tr, batch_size, epochs, use_gpu, print_metrics, init, X_cv, y_cv, print_every)\u001b[0m\n\u001b[0;32m    167\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                     })\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = graph.train(X_tr, y_tr, epochs=5, batch_size=32, use_gpu=False, print_every=1, init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
